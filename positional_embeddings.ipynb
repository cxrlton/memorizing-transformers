{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Rel Pos"
      ],
      "metadata": {
        "id": "9zaIHPrgo3AR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From Memorizing Transformers paper:"
      ],
      "metadata": {
        "id": "vu2bpAQqpZKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Position bias. For dense attention within the local context, we use the T5 relative position bias (Raffel\n",
        "et al., 2020). As noted by Dai et al. (2019), adding a global position encoding to each token does not\n",
        "work well when processing long documents. We don’t use a position bias for the retrieved memories.\n",
        "Experiments on the PG19 dataset (Sun et al., 2021) have shown that relative position does not appear\n",
        "to matter at long range, and the T5 relative bias puts all long-range tokens in the same bucket anyway.\""
      ],
      "metadata": {
        "id": "ohJd9Qb1pPeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From T5 paper:"
      ],
      "metadata": {
        "id": "To8a827OplrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Since self-attention is order-independent (i.e. it is an operation on sets), it is common\n",
        "to provide an explicit position signal to the Transformer. While the original Transformer\n",
        "used a sinusoidal position signal or learned position embeddings, it has recently become\n",
        "more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).\n",
        "Instead of using a fixed embedding for each position, relative position embeddings produce\n",
        "a different learned embedding according to the offset between the “key” and “query” being\n",
        "compared in the self-attention mechanism. We use a simplified form of position embeddings\n",
        "where each “embedding” is simply a scalar that is added to the corresponding logit used\n",
        "for computing the attention weights. For efficiency, we also share the position embedding\n",
        "parameters across all layers in our model, though within a given layer each attention head\n",
        "uses a different learned position embedding. Typically, a fixed number of embeddings are\n",
        "learned, each corresponding to a range of possible key-query offsets. In this work, we use 32\n",
        "embeddings for all of our models with ranges that increase in size logarithmically up to an\n",
        "offset of 128 beyond which we assign all relative positions to the same embedding. Note\n",
        "that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers\n",
        "can build a sensitivity to larger offsets by combining local information from previous layers.\""
      ],
      "metadata": {
        "id": "w5d7leKOppnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RELATIVE POSITION MATRIX\n",
        "# tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13],\n",
        "#         [ -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12],\n",
        "#         [ -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11],\n",
        "#         [ -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10],\n",
        "#         [ -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9],\n",
        "#         [ -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8],\n",
        "#         [ -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7],\n",
        "#         [ -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6],\n",
        "#         [ -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5],\n",
        "#         [ -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4],\n",
        "#         [-10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3],\n",
        "#         [-11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2],\n",
        "#         [-12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1],\n",
        "#         [-13, -12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0]])"
      ],
      "metadata": {
        "id": "HG1ceqOlpsgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Idea\n",
        "\n",
        "- Positional embeddings are added to the QK embeddings during attention\n",
        "- Relative position embeddings identify, for each input example, how far away all the other tokens are from a specific token of interest\n",
        "- Instead of giving each token a relative position index of n that is n positions away from our token of interest, T5 relative position \"buckets\" some tokens into the same index\n",
        "- First we create this set of indices. then the indices are matched to an embedding layer of weight values. These values are then added to the QK embeddings during attention. The positional embeddings are trained with the network.\n",
        "\n",
        "### Recipe\n",
        "\n",
        "- Construct a relative position matrix\n",
        "- For offsets larger than what we want, start to spread offset values logarithmically into a finite amount of buckets. (Past a certian max value (128) we'll just map everything to one value)\n",
        "- Initialize embedding weights that we will assign offset values to\n",
        "- Now the relative position matrix is mapped to these weights\n",
        "- This matrix gets added to our attention when we perform self-attention. Our self-attention now incorporates as a piece of information the relative positions between tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "SjUtdaukrf9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "NMJbDVu7rgQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_buckets = 6 #tot no. of index buckets we'll see\n",
        "max_distance = 20 #max sequence length\n",
        "\n",
        "sequence_length = 14\n",
        "max_context_length = 14"
      ],
      "metadata": {
        "id": "AX6MAJxzrqit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_pos = torch.arange(sequence_length, dtype=torch.long)\n",
        "q_pos = q_pos.reshape(q_pos.shape[0], 1)\n",
        "k_pos = torch.arange(max_context_length, dtype=torch.long)\n",
        "rel_pos = k_pos - q_pos\n",
        "rel_pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXUfoqmvr4OY",
        "outputId": "b5256f3d-0e06-409d-f368-cc553e8e4e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13],\n",
              "        [ -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12],\n",
              "        [ -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11],\n",
              "        [ -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10],\n",
              "        [ -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9],\n",
              "        [ -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8],\n",
              "        [ -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7],\n",
              "        [ -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6],\n",
              "        [ -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5],\n",
              "        [ -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4],\n",
              "        [-10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3],\n",
              "        [-11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2],\n",
              "        [-12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1],\n",
              "        [-13, -12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = -rel_pos\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeAwT-UusLh0",
        "outputId": "7394ba47-ce98-417c-8858-fd49f919b08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11, -12, -13],\n",
              "        [  1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11, -12],\n",
              "        [  2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11],\n",
              "        [  3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10],\n",
              "        [  4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9],\n",
              "        [  5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8],\n",
              "        [  6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7],\n",
              "        [  7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6],\n",
              "        [  8,   7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5],\n",
              "        [  9,   8,   7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4],\n",
              "        [ 10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3],\n",
              "        [ 11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2],\n",
              "        [ 12,  11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0,  -1],\n",
              "        [ 13,  12,  11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = torch.max(n, torch.zeros_like(n))\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhc8NF23sx5g",
        "outputId": "bdbaf946-5f58-44c4-c996-be74a8a0a646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 3,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0],\n",
              "        [ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0],\n",
              "        [10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0],\n",
              "        [11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0],\n",
              "        [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0],\n",
              "        [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# half of the buckets are for exact increments in positions\n",
        "max_exact = num_buckets // 2\n",
        "max_exact"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhjn0LLbs6o_",
        "outputId": "cf5f898a-0334-4a95-c55c-c19178ef0e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_small = n < max_exact\n",
        "is_small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlJ4F5rGs_8x",
        "outputId": "c58d16f3-8e04-4046-d0b8-ee87fa75d335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False, False,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False, False, False,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False, False, False, False,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False, False, False, False,\n",
              "         False,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other half of the buckets are for logarithmically bigger bins in positions up to max_distance.\n",
        "\n",
        "So, we map a positional embeddings up to a number k exactly (offset by 1, offset by 2, offset by 3...) but at a certain point we have a longer sequence than positional embedding \"buckets\" (like bins), so we map them logarithmically to spread out over our fixed number of buckets: e.g. [1,2,3,4,5,5,6,6,7,7,7,8,8,8,8,]"
      ],
      "metadata": {
        "id": "ABbxOXL0ttph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_if_large = max_exact + \\\n",
        "  (\n",
        "    torch.log(n.float() / max_exact)  # log of matrix divided by scalar\n",
        "    / math.log(max_distance / max_exact) * (num_buckets - max_exact) # scalar\n",
        "    ).long() # convert float to int\n",
        "\n",
        "val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact))"
      ],
      "metadata": {
        "id": "kLW47jUztu57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_if_large"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnQdAv4_t1RG",
        "outputId": "43d5f648-4879-47a8-86b6-6a1462214b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [1.2627,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [2.3588, 1.2627,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [3.0000, 2.3588, 1.2627,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [3.4549, 3.0000, 2.3588, 1.2627,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588,\n",
              "         1.2627,   -inf,   -inf,   -inf,   -inf],\n",
              "        [5.0546, 4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000,\n",
              "         2.3588, 1.2627,   -inf,   -inf,   -inf],\n",
              "        [5.1922, 5.0546, 4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549,\n",
              "         3.0000, 2.3588, 1.2627,   -inf,   -inf],\n",
              "        [5.3188, 5.1922, 5.0546, 4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078,\n",
              "         3.4549, 3.0000, 2.3588, 1.2627,   -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_if_large = val_if_large.long()\n",
        "val_if_large"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvbahiEFt3ev",
        "outputId": "f74311a5-183d-42b0-c1e6-9502a8c16b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   1, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   2,                    1, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   3,                    2,                    1,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   3,                    3,                    2,\n",
              "                            1, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   3,                    3,                    3,\n",
              "                            2,                    1, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    3,                    3,\n",
              "                            3,                    2,                    1,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    4,                    3,\n",
              "                            3,                    3,                    2,\n",
              "                            1, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    4,                    4,\n",
              "                            3,                    3,                    3,\n",
              "                            2,                    1, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    4,                    4,\n",
              "                            4,                    3,                    3,\n",
              "                            3,                    2,                    1,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    4,                    4,\n",
              "                            4,                    4,                    3,\n",
              "                            3,                    3,                    2,\n",
              "                            1, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   5,                    4,                    4,\n",
              "                            4,                    4,                    4,\n",
              "                            3,                    3,                    3,\n",
              "                            2,                    1, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   5,                    5,                    4,\n",
              "                            4,                    4,                    4,\n",
              "                            4,                    3,                    3,\n",
              "                            3,                    2,                    1,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   5,                    5,                    5,\n",
              "                            4,                    4,                    4,\n",
              "                            4,                    4,                    3,\n",
              "                            3,                    3,                    2,\n",
              "                            1, -9223372036854775808]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position_bucket_indices = torch.where(is_small, n, val_if_large)\n",
        "position_bucket_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtT1J3rEx8X1",
        "outputId": "ac5b5213-04cd-41e2-e112-113691e09e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0],\n",
              "        [4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0],\n",
              "        [5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0],\n",
              "        [5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0],\n",
              "        [5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "heads = 4\n",
        "relative_position_bias = nn.Embedding(num_buckets, heads)\n",
        "relative_position_bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVY_8ijnyIDY",
        "outputId": "fb8dd720-80b8-4b4a-c50c-a6dfe966803b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(6, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relative_position_bias.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xs0MwyszsXP",
        "outputId": "f1fbd2e5-5cd9-47dd-9017-687637ec1385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.7429,  1.0904,  0.8333,  0.2273],\n",
              "        [-0.4354,  1.5028,  1.2448,  0.7483],\n",
              "        [-0.0872, -1.7964,  0.8769, -0.3345],\n",
              "        [ 2.7251,  1.2114, -0.8605, -0.7477],\n",
              "        [-0.9158, -1.1318, -1.0841, -1.7824],\n",
              "        [ 0.1928,  0.4019,  2.2828, -1.8319]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relative_position_values = relative_position_bias(position_bucket_indices)\n",
        "relative_position_values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieRFSWjO0H0h",
        "outputId": "0105e499-e99e-4fed-aae9-05411ab85dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14, 14, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to reshape from (sequence, context, heads) -> (batch, heads, sequence, context)\n",
        "relative_position_values = relative_position_values.transpose(0,2).unsqueeze(0)\n",
        "relative_position_values.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQYc4rbJ0nkX",
        "outputId": "f550d563-c9ab-4a29-d3b1-c49679571935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 14, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it all in a class:"
      ],
      "metadata": {
        "id": "vY0CED3J1lZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RelativePosition(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      rp_scale,\n",
        "      num_buckets = 32,\n",
        "      rp_max_distance = 128,\n",
        "      heads = 8\n",
        "  ):\n",
        "      super().__init__()\n",
        "      self.scale = rp_scale\n",
        "      self.num_buckets = num_buckets\n",
        "      self.rp_max_distance = rp_max_distance\n",
        "      self.relative_attention_embedding = nn.Embedding(num_buckets, heads)\n",
        "\n",
        "  def relative_position_bucket(self, relative_position_matrix):\n",
        "      n = -relative_position_matrix\n",
        "      n = torch.max(n, torch.zeros_like(n))\n",
        "\n",
        "      max_exact = self.num_buckets // 2\n",
        "\n",
        "      is_small = n < max_exact\n",
        "      val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(self.rp_max_distance / max_exact) * (self.num_buckets - max_exact)).long()\n",
        "      val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, self.num_buckets - 1))\n",
        "\n",
        "      return torch.where(is_small, n, val_if_large)\n",
        "\n",
        "  def forward(self, sequence_length, max_context_length):\n",
        "\n",
        "      sequence_pos = torch.arange(sequence_length, dtype=torch.long)\n",
        "      context_pos = torch.arange(max_context_length, dtype=torch.long)\n",
        "      sequence_pos = sequence_pos.reshape(sequence_pos.shape[0], 1)\n",
        "      rel_pos = context_rel_pos - sequence_rel_pos\n",
        "\n",
        "      position_bucket_indices = self.relative_position_bucket(rel_pos)\n",
        "\n",
        "      rp_values = self.relative_attention_embedding(position_bucket_indices)\n",
        "      # Rearrange (sequence, context, heads) -> (1, heads, sequence, context)\n",
        "      rp_values = rp_values.transpose(0,2)\n",
        "      rp_values = rp_values.unsqueeze(0)\n",
        "      return rp_values * self.scale"
      ],
      "metadata": {
        "id": "dmUyGaNl1ZVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NTflNRuV1hb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}